{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457785c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from docarray import BaseDoc, DocList  # corrigindo nomes modernos\n",
    "from typing import List\n",
    "from numpy.typing import NDArray\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pickle, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375e2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VIZINHOS = 5  \n",
    "\n",
    "DIRETORIO_SAIDA = \"indices_chatbot\"\n",
    "NOME_ARQUIVO_INDICE = \"faiss_index_{cultura}.index\"\n",
    "NOME_ARQUIVO_METADADOS = \"metadata_{cultura}.json\"\n",
    "\n",
    "# Garante que o diret√≥rio para salvar os arquivos exista\n",
    "os.makedirs(DIRETORIO_SAIDA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fed1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulks = {\n",
    "    \"abacaxi\": \"bulk/bulk-abacaxi.json\",\n",
    "    \"uva\": \"bulk/bulk-uva.json\",\n",
    "    \"milho\": \"bulk/bulk-milho.json\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8f35f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  glob import glob\n",
    "glob('data/book/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e826c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sintik/sadai-chat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86546912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Doc(BaseDoc):\n",
    "    text: str\n",
    "    index: str | None = None\n",
    "    question_number: int | None = None\n",
    "    question: str | None = None\n",
    "    answer: str | None = None\n",
    "    chapter: str | None = None\n",
    "    book: str | None = None\n",
    "    book_id: str | None = None\n",
    "    epub: str | None = None\n",
    "    pdf: str | None = None\n",
    "    html: str | None = None\n",
    "    year: str | None = None\n",
    "    embedding: list[float] | None = None\n",
    "    \n",
    "\n",
    "def carregar_doc(jsonl_path, cultura=None):\n",
    "    docs = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # l√™ as linhas de 2 em 2 (index + data)\n",
    "    for i in range(0, len(lines), 2):\n",
    "        index_line = json.loads(lines[i])\n",
    "        data_line = json.loads(lines[i + 1])\n",
    "\n",
    "        doc = QA_Doc(\n",
    "            text=f\"{data_line.get('question', '')}\\n{data_line.get('answer', '')}\",\n",
    "            index=index_line[\"index\"][\"_id\"],\n",
    "            question_number=data_line.get(\"question_number\"),\n",
    "            question=data_line.get(\"question\"),\n",
    "            answer=data_line.get(\"answer\"),\n",
    "            chapter=data_line.get(\"chapter\"),\n",
    "            book=data_line.get(\"book\"),\n",
    "            book_id=data_line.get(\"book_id\"),\n",
    "            epub=data_line.get(\"epub\"),\n",
    "            pdf=data_line.get(\"pdf\"),\n",
    "            year=str(data_line.get(\"year\")) if data_line.get(\"year\") is not None else \"\",\n",
    "        )\n",
    "\n",
    "        # n√£o usamos 'doc.cultura' mais\n",
    "        # doc.cultura = cultura\n",
    "        # doc.question = data_line.get(\"question\", \"\")  # j√° est√° setado no construtor\n",
    "\n",
    "        docs.append(doc)\n",
    "\n",
    "    return DocList[QA_Doc](docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1407bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Carregando documentos de abacaxi...\n",
      " Carregando documentos de uva...\n",
      " Carregando documentos de milho...\n",
      "Cultura: abacaxi -> 500 documentos\n",
      "Exemplo: A falta de chuva prejudica o abacaxizeiro?\n",
      "\n",
      "Cultura: uva -> 500 documentos\n",
      "Exemplo: Quais s√£o os m√©todos usados no melhoramento gen√©tico da videira?\n",
      "\n",
      "Cultura: milho -> 500 documentos\n",
      "Exemplo: Como o clima influencia a cultura do milho?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_cultura = defaultdict(list)\n",
    "\n",
    "for cultura, caminho in bulks.items():\n",
    "    print(f\" Carregando documentos de {cultura}...\")\n",
    "    docs = carregar_doc(caminho, cultura)\n",
    "    doc_cultura[cultura].extend(docs)\n",
    "\n",
    "# Mostra resumo de quantos docs por cultura\n",
    "for cultura, docs in doc_cultura.items():\n",
    "    print(f\"Cultura: {cultura} -> {len(docs)} documentos\")\n",
    "    if len(docs) > 0:\n",
    "        print(\"Exemplo:\", docs[0].question)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrutura 'doc_cultura' salva em ./dados/doc_cultura.pkl\n"
     ]
    }
   ],
   "source": [
    "# Garante diret√≥rio de sa√≠da\n",
    "os.makedirs(\"./dados\", exist_ok=True)\n",
    "\n",
    "# Caminho do arquivo\n",
    "CAMINHO_PKL = \"./dados/doc_cultura.pkl\"\n",
    "\n",
    "# Salva a estrutura de documentos (persist√™ncia)\n",
    "with open(CAMINHO_PKL, \"wb\") as f:\n",
    "    pickle.dump(doc_cultura, f)\n",
    "\n",
    "print(f\"Estrutura 'doc_cultura' salva em {CAMINHO_PKL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b3987fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_cultura carregado com 3 culturas.\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(doc_cultura, dict) or len(doc_cultura) == 0:\n",
    "    raise ValueError(\" doc_cultura est√° vazio ou inv√°lido!\")\n",
    "else:\n",
    "    print(f\"doc_cultura carregado com {len(doc_cultura)} culturas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19141f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cultura: abacaxi ===\n",
      "Criando novo √≠ndice...\n",
      "√çndice e metadados salvos.\n",
      "\n",
      "=== Cultura: uva ===\n",
      "Criando novo √≠ndice...\n",
      "√çndice e metadados salvos.\n",
      "\n",
      "=== Cultura: milho ===\n",
      "Criando novo √≠ndice...\n",
      "√çndice e metadados salvos.\n",
      "\n",
      "‚úì Todos os √≠ndices FAISS foram carregados ou criados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cria√ß√£o e persist√™ncia dos √≠ndices FAISS usando SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"PORTULAN/serafim-335m-portuguese-pt-sentence-encoder-ir\"\n",
    ")\n",
    "\n",
    "def gerar_embeddings(textos):\n",
    "    \"\"\"\n",
    "    Gera embeddings normalizados usando SentenceTransformer.\n",
    "    J√° retorna float32 normalizado (N-esfera).\n",
    "    \"\"\"\n",
    "    emb = model.encode(\n",
    "        textos,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return emb.astype(\"float32\")\n",
    "\n",
    "# === Diret√≥rio onde os √≠ndices ser√£o salvos ===\n",
    "DIRETORIO_SAIDA = \"./indices_faiss\"\n",
    "os.makedirs(DIRETORIO_SAIDA, exist_ok=True)\n",
    "\n",
    "NOME_ARQUIVO_INDICE = \"index_{cultura}.faiss\"\n",
    "NOME_ARQUIVO_METADADOS = \"metadata_{cultura}.json\"\n",
    "\n",
    "# √çndices FAISS por cultura\n",
    "faiss_index_cultura = {}\n",
    "metadados_cultura = {}\n",
    "\n",
    "for cultura, docs in doc_cultura.items():\n",
    "\n",
    "    print(f\"\\n=== Cultura: {cultura} ===\")\n",
    "\n",
    "    caminho_indice = os.path.join(DIRETORIO_SAIDA, NOME_ARQUIVO_INDICE.format(cultura=cultura))\n",
    "    caminho_meta   = os.path.join(DIRETORIO_SAIDA, NOME_ARQUIVO_METADADOS.format(cultura=cultura))\n",
    "\n",
    "    # Se j√° existir, s√≥ carrega\n",
    "    if os.path.exists(caminho_indice) and os.path.exists(caminho_meta):\n",
    "        print(\"Carregando √≠ndice existente...\")\n",
    "\n",
    "        index = faiss.read_index(caminho_indice)\n",
    "        faiss_index_cultura[cultura] = index\n",
    "\n",
    "        with open(caminho_meta, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadados_cultura[cultura] = json.load(f)\n",
    "\n",
    "        continue\n",
    "\n",
    "    # Caso contr√°rio, cria do zero\n",
    "    print(\"Criando novo √≠ndice...\")\n",
    "\n",
    "    textos = [f\"{doc.question} {doc.answer}\" for doc in docs]\n",
    "    embeddings = gerar_embeddings(textos)\n",
    "\n",
    "    dimensao = embeddings.shape[1]\n",
    "    index_flat = faiss.IndexFlatL2(dimensao)\n",
    "    index = faiss.IndexIDMap(index_flat)\n",
    "\n",
    "    ids = np.arange(len(docs)).astype(\"int64\")\n",
    "    index.add_with_ids(embeddings, ids)\n",
    "    faiss_index_cultura[cultura] = index\n",
    "\n",
    "    # Salva √≠ndice FAISS\n",
    "    faiss.write_index(index, caminho_indice)\n",
    "\n",
    "    # Salva metadados\n",
    "    metadados = {\n",
    "        str(i): {\n",
    "            \"question\": doc.question,\n",
    "            \"answer\": doc.answer,\n",
    "            \"book\": doc.book,\n",
    "        }\n",
    "        for i, doc in enumerate(docs)\n",
    "    }\n",
    "\n",
    "    with open(caminho_meta, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadados, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    metadados_cultura[cultura] = metadados\n",
    "\n",
    "    print(\"√çndice e metadados salvos.\")\n",
    "\n",
    "print(\"\\n‚úì Todos os √≠ndices FAISS foram carregados ou criados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fun√ß√£o para detectar a cultura mais pr√≥xima\n",
    "\n",
    "\n",
    "def detectar_cultura(query, embeddings_por_cultura):\n",
    "    # 1. Calcula a m√©dia dos embeddings de cada cultura\n",
    "    medias_cultura = {\n",
    "        cultura: np.mean(np.array(embs), axis=0)\n",
    "        for cultura, embs in embeddings_por_cultura.items()\n",
    "    }\n",
    "\n",
    "    # 2. Gera o embedding da query usando o modelo Serafim\n",
    "    query_emb = gerar_embeddings([query])[0] \n",
    "\n",
    "    # 3. Calcula similaridade coseno entre query e m√©dias\n",
    "    similaridades = {}\n",
    "    for cultura, media in medias_cultura.items():\n",
    "        media_norm = media / np.linalg.norm(media)\n",
    "        sim = np.dot(query_emb, media_norm)\n",
    "        similaridades[cultura] = sim\n",
    "\n",
    "    # 4. Determina cultura mais pr√≥xima\n",
    "    cultura_mais_proxima = max(similaridades, key=similaridades.get)\n",
    "\n",
    "    \n",
    "    print(\" Similaridades por cultura:\")\n",
    "    for c, s in similaridades.items():\n",
    "        print(f\"   {c}: {s:.4f}\")\n",
    "    print(f\" Cultura mais pr√≥xima: {cultura_mais_proxima}\")\n",
    "\n",
    "    return cultura_mais_proxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ef4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Buscar documentos apenas na cultura mais pr√≥xima\n",
    "\n",
    "def buscar_documentos_por_cultura(query, embeddings_por_cultura, k=5):\n",
    "    # Detecta cultura mais pr√≥xima\n",
    "    cultura_escolhida = detectar_cultura(query, embeddings_por_cultura)\n",
    "\n",
    "    # Recupera √≠ndice e documentos da cultura\n",
    "    index = faiss_index_cultura[cultura_escolhida]\n",
    "    docs_cultura = doc_cultura[cultura_escolhida]\n",
    "\n",
    "    # Gera embedding da query\n",
    "    query_emb = gerar_embeddings([query]).astype('float32')\n",
    "    faiss.normalize_L2(query_emb)\n",
    "\n",
    "    # Busca no √≠ndice FAISS\n",
    "    D, I = index.search(query_emb, k)\n",
    "    print(f\"\\n Resultados da busca na cultura: {cultura_escolhida}\\n\")\n",
    "\n",
    "    # Monta resultados leg√≠veis\n",
    "    resultados = []\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        if idx < len(docs_cultura):\n",
    "            pergunta = docs_cultura[idx].tags.get(\"question\", \"\")\n",
    "            resposta = docs_cultura[idx].tags.get(\"answer\", \"\")\n",
    "            resultados.append({\n",
    "                \"pergunta\": pergunta,\n",
    "                \"resposta\": resposta,\n",
    "                \"distancia\": float(dist)\n",
    "            })\n",
    "            print(f\"üîπ {pergunta} (dist√¢ncia={dist:.4f})\")\n",
    "\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09058fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fun√ß√£o de busca sem√¢ntica implementada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o de Busca Sem√¢ntica entre todas as culturas\n",
    "\n",
    "\n",
    "def buscar_documentos_relevantes(query, k=5):\n",
    "    # Gera embedding da query\n",
    "    query_emb = gerar_embeddings([query]).astype('float32')\n",
    "    resultados = []\n",
    "\n",
    "    #  Busca em cada √≠ndice FAISS por cultura\n",
    "    for cultura, index in faiss_index_cultura.items():\n",
    "        D, I = index.search(query_emb, k)\n",
    "\n",
    "        for idx, distancia in zip(I[0], D[0]):\n",
    "            if idx < len(doc_cultura[cultura]):  # Verifica √≠ndice v√°lido\n",
    "                doc = doc_cultura[cultura][idx]\n",
    "                resultados.append({\n",
    "                    'doc': doc,\n",
    "                    'cultura': cultura,\n",
    "                    'distancia': float(distancia)\n",
    "                })\n",
    "\n",
    "    # Ordena os resultados por dist√¢ncia (quanto menor, mais pr√≥ximo)\n",
    "    resultados.sort(key=lambda x: x['distancia'])\n",
    "\n",
    "    # Seleciona os top-k documentos mais pr√≥ximos\n",
    "    top_docs = [r['doc'] for r in resultados[:k]]\n",
    "\n",
    "    #  Determina cultura predominante entre os top-k\n",
    "    if top_docs:\n",
    "        culturas_resultados = [r['cultura'] for r in resultados[:k]]\n",
    "        cultura_predominante = max(set(culturas_resultados), key=culturas_resultados.count)\n",
    "    else:\n",
    "        cultura_predominante = \"abacaxi\"  \n",
    "\n",
    "    print(f\" Cultura predominante: {cultura_predominante}\")\n",
    "    return top_docs, cultura_predominante\n",
    "\n",
    "print(\" Fun√ß√£o de busca sem√¢ntica implementada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b480d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gerando embeddings para abacaxi...\n",
      " 500 embeddings criados para abacaxi\n",
      "\n",
      " Gerando embeddings para uva...\n",
      " 500 embeddings criados para uva\n",
      "\n",
      " Gerando embeddings para milho...\n",
      " 500 embeddings criados para milho\n",
      "\n",
      " Embeddings coletados para todas as culturas!\n",
      "M√©dias de embeddings calculadas!\n"
     ]
    }
   ],
   "source": [
    "# Dicion√°rio para guardar embeddings m√©dios de cada cultura para detec√ß√£o de cultura\n",
    "embeddings_por_cultura = {}\n",
    "\n",
    "for cultura, docs in doc_cultura.items():\n",
    "    if len(docs) == 0:\n",
    "        print(f\" Nenhum documento encontrado para {cultura}, pulando...\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Gerando embeddings para {cultura}...\")\n",
    "    textos = [f\"{doc.question} {doc.answer}\" for doc in docs]\n",
    "    embeddings = gerar_embeddings(textos)  # usa a MESMA fun√ß√£o que o FAISS usa\n",
    "    embeddings_por_cultura[cultura] = embeddings\n",
    "    print(f\" {len(embeddings)} embeddings criados para {cultura}\\n\")\n",
    "\n",
    "print(\" Embeddings coletados para todas as culturas!\")\n",
    "\n",
    "\n",
    "# Calcula a m√©dia por cultura (para detectar cultura mais pr√≥xima)\n",
    "medias_cultura = {\n",
    "    cultura: np.mean(embeds, axis=0)\n",
    "    for cultura, embeds in embeddings_por_cultura.items()\n",
    "}\n",
    "print(\"M√©dias de embeddings calculadas!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2564b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
